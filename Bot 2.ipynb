{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE48q2qOdK-k"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgcdlvdug-3f"
      },
      "source": [
        "# read source file\n",
        "file = open(\"plots_text.pickle\",\"rb\")\n",
        "corpus = file.load(file)\n",
        "\n",
        "# count of movie plot summaries\n",
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnv-pWCEfE_2"
      },
      "source": [
        "# clean text\n",
        "cleeaned = [re.sub(\"[^a-z' ]\", \"\", i) for i in corpus]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-wHEWrdipGS"
      },
      "source": [
        "# create sequences of length 5 tokens\n",
        "def create_seq(text, seq_len = 5):\n",
        "    \n",
        "    sequences = []\n",
        "\n",
        "        # if the number of tokens in 'text' is greater than 5\n",
        "        if len(text.split()) > seq_len:\n",
        "        for i in range(seq_len, len(text.split())):\n",
        "        # select sequence of tokens\n",
        "        seq = text.split()[i-seq_len:i+1]\n",
        "        # add to the list\n",
        "        sequences.append(\" \".join(seq))\n",
        "\n",
        "        return sequences\n",
        "\n",
        "        # if the number of tokens in 'text' is less than or equal to 5\n",
        "       else:\n",
        "      \n",
        "      return [text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVyFql3Ni_xg"
      },
      "source": [
        "# create integer-to-token mapping\n",
        "int2token = {}\n",
        "cnt = 0\n",
        "\n",
        "for w in set(\" \".join(corpus).split()):\n",
        "  int2token[cnt] = w\n",
        "  cnt+= 1\n",
        "\n",
        "# create token-to-integer mapping\n",
        "token2int = {t: i for i, t in int2token.items()}\n",
        "\n",
        "token2int[\"the\"], int2token[14271]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMdKFEZJmFD6"
      },
      "source": [
        "# set vocabulary size\n",
        "vocab_size = len(int2token)\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMKteOQOmXFO"
      },
      "source": [
        "def get_integer_seq(seq):\n",
        "  return [token2int[w] for w in seq.split()]\n",
        "\n",
        "# convert text sequences to integer sequences\n",
        "x_int = [get_integer_seq(i) for i in x]\n",
        "y_int = [get_integer_seq(i) for i in y]\n",
        "\n",
        "# convert lists to numpy arrays\n",
        "x_int = np.array(x_int)\n",
        "y_int = np.array(y_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gJpjMRHmoGR"
      },
      "source": [
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "         \n",
        "    # iterate through the arrays\n",
        "    prv = 0\n",
        "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "      x = arr_x[prv:n,:]\n",
        "      y = arr_y[prv:n,:]\n",
        "      prv = n\n",
        "      yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLH8NiaRq7Ju"
      },
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## define the fully-connected layer\n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "        ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)     \n",
        "        \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "        \n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(lstm_output)\n",
        "        \n",
        "        #out = out.contiguous().view(-1, self.n_hidden) \n",
        "        out = out.reshape(-1, self.n_hidden) \n",
        "\n",
        "        ## put \"out\" through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oqqdJwq99-"
      },
      "source": [
        "def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        # if GPU is available\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        \n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGJJKsZYrQBA"
      },
      "source": [
        "# instantiate the model\n",
        "net = WordLSTM()\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag2p_kWPrXTe"
      },
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
        "    \n",
        "    # optimizer\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    \n",
        "    # loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # # push model to GPU\n",
        "    # net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(x_int, y_int, batch_size):\n",
        "            counter+= 1\n",
        "            \n",
        "            # convert numpy arrays to PyTorch arrays\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            # # push tensors to GPU\n",
        "            # inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # detach hidden states\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "\n",
        "            # back-propagate error\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            # update weigths\n",
        "            opt.step()            \n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "            \n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTViFhFEstQu"
      },
      "source": [
        "# train the model\n",
        "train(net, batch_size = 32, epochs=20, print_every=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMqe947EtCAZ"
      },
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FF4SDQJtD7y"
      },
      "source": [
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "\n",
        "    # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h\n",
        "\n",
        "  # function to generate text\n",
        "def sample(net, size, prime='it is'):\n",
        "        \n",
        "    # # push to GPU\n",
        "    # net.cuda()\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(net, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY_fCRSkuldF"
      },
      "source": [
        "#sample with default prime\n",
        "sample(net, 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp8j19Ttuqcl"
      },
      "source": [
        "#sample with different prime\n",
        "sample(net, 15, prime = \"one of the\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtAtxI6jvXoL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}