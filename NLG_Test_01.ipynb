{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legendary-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "random-librarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read pickle file\n",
    "pickle_in = open(\"plots_text.pickle\",\"rb\")\n",
    "movie_plots = pickle.load(pickle_in)\n",
    "\n",
    "# count of movie plot summaries\n",
    "len(movie_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intelligent-pizza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the movie starts with a loner named narendran  writing a suicide note. he mentions that he is committing suicide as he has nothing to do with his life. he chooses to jump in front of the train to perform the act. he waits for the train but end up saving another young man named janardanan  who has the same intention. janardanan has committed a murder out of desperation and is scared of revenge. narendran consoles him and offers him to take the blame of the murder so that janardanan can live freely. narendran gets sentenced to lifetime imprisonment. while in jail, he writes a book under the pen name \"ben\" which becomes immensely popular. he is also awarded by the state, but his true identity is never revealed. journalist manasa  discovers that the author is behind bars and tries to get a parole for him. though narendran is not initially interested, he eventually applies for parole and is granted 28 days of parole so that he can receive the award in person. once out of jail, narendran gets word that janardanan has died mysteriously. he sets out on a mission to find the culprits and destroy those who caused his death.',\n",
       " 'a mongolian tale tells the story of two childhood sweethearts from their youth into their adulthood as set on the mongolian steppes. nai nai , an old mongolian woman is living with her orphaned granddaughter, someyer, when she accepts into her home a boy, beiyinpalica, whose mother has died and father cannot care for. though raised as a brother and sister, somiya and beiyinpalica grow close. as beiyinpalica (now played by the mongolian pop singer [[tengger  is about to enter adulthood, his father suddenly writes and orders him into the city to study veterinary science. he leaves but promises someyer that he will marry her when he returns. while in the city, beiyinpalica also becomes a student of music. when he returns three years later, he discovers that someyer  has become pregnant by another man. heartbroken, he leaves once more. twelve years later, beiyinpalica has become a celebrated folk singer. someyer, meanwhile, has born four sons and one daughter, though she is generally neglected by her drunken husband. back home, beiyinpalica suddenly finds himself in the position of being a father-figure for the girl.',\n",
       " 'kutty ([[vijay  wants to become a singer while working as a local cable provider run by mani . his songs are appreciated by a college student rukmani ([[simran  and each time when she wishes to meet him, circumstances project him as a transitive element. he also becomes the cause for rukmani losing her eyesight and repents for it. he starts to love her and he regularly writes to his mother about the development of love he has for rukmani. when his mother dies, she offers her eyes to rukmani. to meet the cost of the eye transplant, kutty offers his kidney to a pune-based richman. while returning home he inadvertently becomes an accomplice in creating explosives and he is arrested. having regained her vision, rukmani, who had studied for ias, becomes a collector and when kutty tries to contact her, he is still the rowdy element in her mind. finally the confusion is cleared and the lovers can be together.',\n",
       " \"jim nashe worked as a fireman, but a large inheritance and a divorce from his wife has left him free to buy a new car and see the country at his leisure. he picks up a hitchhiker, jack pozzi, who turns out to be a professional gambler. pozzi tells how he just lost $10,000 to a pair of eccentric old millionaires in a poker game, simply through a bad stroke of luck. with money and time to spare, the intrigued nashe offers to back pozzi with $10,000 for a rematch. the wealthy men, flower and stone, live together on a huge estate. they willingly agree to another game, but are not the suckers pozzi takes them for. they win again. nashe puts up his car as collateral against the cash if flower and stone will cut cards, winner take all. nashe loses. no clear alternative exists except to work off the debt. the quirky flower and stone have a pile of 10,000 heavy stones, said to be from a 15th century castle originally. they would like to build a wall on their property, so they tell nashe and pozzi that if they devote the next 50 days to erecting the wall, their debt will be paid in full. a foreman named calvin murks keeps an eye on the two men. nashe methodically goes about his task, carrying the stone, but pozzi becomes increasingly unhinged, feeling like a slave. pozzi takes offense at a snide remark by murks and assaults him, whereupon murks begins coming to work armed with a gun. pozzi attempts to escape. his badly beaten body is found. nashe can't be sure who did this, although murks seems a likely suspect. pozzi eventually disappears and nashe fears that he's dead. he privately decides to get even, the best way he knows how.\",\n",
       " 'off the southern coast of the korean peninsula, an island of 17 inhabitants exists. the so-called paradise island holds up to its name with its breath-taking mountains and sea coupled with good-natured people. no worries or stress holds for anyone who comes to visit this beautiful oasis. but this peace doesnâ€™t last long as every single one of the inhabitants disappear one day without a single trace. chaos initially breaks out when a blood-drenched corpse is found and everyone becomes a suspect. the furious sea allows them no boat ride to the mainland and their only existing radio communication device has been smashed. trapped together on the island, everyone is suspicious of each other and even the unseen could be a possible suspect. as hideous secrets get revealed day by day, an island of paradise slowly turns into an island of death.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample random summaries\n",
    "random.sample(movie_plots, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "close-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "movie_plots = [re.sub(\"[^a-z' ]\", \"\", i) for i in movie_plots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "systematic-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(text, seq_len = 5):\n",
    "    \n",
    "    sequences = []\n",
    "\n",
    "    # if the number of tokens in 'text' is greater than 5\n",
    "    if len(text.split()) > seq_len:\n",
    "      for i in range(seq_len, len(text.split())):\n",
    "        # select sequence of tokens\n",
    "        seq = text.split()[i-seq_len:i+1]\n",
    "        # add to the list\n",
    "        sequences.append(\" \".join(seq))\n",
    "\n",
    "      return sequences\n",
    "\n",
    "    # if the number of tokens in 'text' is less than or equal to 5\n",
    "    else:\n",
    "      \n",
    "      return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "royal-morocco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152644"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [create_seq(i) for i in movie_plots]\n",
    "\n",
    "# merge list-of-lists into a single list\n",
    "seqs = sum(seqs, [])\n",
    "\n",
    "# count of sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aware-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "digital-louisiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7088, 'comment')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "for w in set(\" \".join(movie_plots).split()):\n",
    "  int2token[cnt] = w\n",
    "  cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}\n",
    "\n",
    "token2int[\"the\"], int2token[14271]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "later-corrections",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16592"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set vocabulary size\n",
    "vocab_size = len(int2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sweet-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integer_seq(seq):\n",
    "  return [token2int[w] for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x_int = [get_integer_seq(i) for i in x]\n",
    "y_int = [get_integer_seq(i) for i in y]\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "x_int = np.array(x_int)\n",
    "y_int = np.array(y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oriented-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "         \n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "      x = arr_x[prv:n,:]\n",
    "      y = arr_y[prv:n,:]\n",
    "      prv = n\n",
    "      yield x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moved-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incorrect-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ruled-alaska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(16592, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=16592, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = WordLSTM()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "# net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bacterial-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to GPU\n",
    "#     net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "            \n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vulnerable-entertainment",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordLSTM' object has no attribute 'init_hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-27b512b92203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-20c98b64c83b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, epochs, batch_size, lr, clip, print_every)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# initialize hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    946\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 948\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WordLSTM' object has no attribute 'init_hidden'"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-capture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
